

## ğŸ§  What You Are Building

A system where:

- Documents are uploaded to Google Drive  
- Text is extracted from documents  
- Text is converted into vector embeddings  
- Embeddings are stored in a vector database  
- A user asks a question  
- AI answers using **only the uploaded documents**

---

## ğŸ§° Accounts Required (Free Tier)

Create accounts for the following services:

- â˜ï¸ **n8n Cloud**  
- ğŸ“ **Google Drive**  
- ğŸ¤— **Hugging Face** (Read access token)  
- ğŸ§  **Qdrant Cloud** (Free cluster)  
- âœ¨ **Google Gemini API**

---

## ğŸ§° Tech Stack

| Component | Tool |
|---------|------|
| Workflow Automation | n8n Cloud |
| Document Storage | Google Drive |
| Text Extraction | n8n Default Data Loader |
| Embeddings | Hugging Face (`all-MiniLM-L6-v2`) |
| Vector Database | Qdrant Cloud |
| LLM | Google Gemini |
| Framework | LangChain (via n8n nodes) |

---

## ğŸ”¹ STEP 1: Create Qdrant Collection

Create a collection in Qdrant with the following configuration:

- **Collection Name:** `my_collection`  
- **Vector Size:** `384`  
- **Distance Metric:** `Cosine`  

> âš ï¸ Vector size must exactly match the embedding model output.

---

## ğŸ”¹ STEP 2: Document Ingestion Workflow

This workflow ingests documents from Google Drive and stores them in the vector database.

### ğŸ”— Nodes Used

1. Manual Trigger  
2. Google Drive â€“ Search Files  
3. Google Drive â€“ Download File  
4. Default Data Loader  
5. Hugging Face Embeddings  
6. Qdrant Vector Store (Insert Mode)

---

### ğŸ“¥ Google Drive â€“ Download File

- **Output:** Binary file  
- **Binary Property:** `data`  

This node downloads the document from Google Drive.

---

### ğŸ“„ Default Data Loader

- **Data Type:** `binary`  
- Automatically extracts text from:
  - DOCX  
  - PDF  
  - TXT  

This step converts documents into readable text.

---

### ğŸ”¢ Hugging Face Embeddings

- **Model:** `sentence-transformers/all-MiniLM-L6-v2`  
- **Embedding Dimension:** `384`  

This node converts text into numerical vectors (embeddings).

---

### ğŸ—„ï¸ Qdrant Vector Store (Insert)

- **Mode:** Insert  
- **Collection:** `my_collection`  

Receives:
- Documents from **Default Data Loader**  
- Embeddings from **Hugging Face**

After this step, documents are indexed and searchable.

---

## ğŸ”¹ STEP 3: Question Answering Workflow

This workflow allows users to ask questions and receive answers.

### ğŸ”— Nodes Used

1. Chat Trigger  
2. Hugging Face Embeddings (Query)  
3. Qdrant Vector Store (Search Mode)  
4. Aggregate  
5. AI Agent  
6. Google Gemini Chat Model  

---

## ğŸ§  AI Agent Prompt (IMPORTANT)

Use **this exact prompt** in the AI Agent node:

```text
You are a helpful assistant.
Use the provided context to answer the user's question.
If the answer is not contained in the context, say "I don't know".

Context:
{{ $json.pageContent }}

Question:
{{ $('When chat message received').item.json.chatInput }}

Answer:


```

## ğŸ”¹ STEP 4: Testing the System

Try asking questions such as:

- â“ What is mentioned about the admission process?
- ğŸ“„ Summarize the uploaded document
- ğŸ”‘ What are the key points discussed?

---

## âš ï¸ Common Mistakes to Avoid

- Skipping the **Default Data Loader**
- Using different embedding models for ingestion and querying
- Sending full documents directly to the LLM
- Forgetting to re-ingest documents after updates

---

## âœ… Best Practices

- ğŸ“ Keep chunk size between **300â€“600 characters**
- ğŸ” Use the **same embedding model** everywhere
- ğŸ” Store API keys using **n8n credentials**
- â™»ï¸ Re-run ingestion whenever documents change

---

## ğŸ¯ Outcome

After completing this project:

- You understand **RAG architecture**
- You can build **production-grade AI workflows**
- You can extend this system for **clients or products**

---

## ğŸš€ Future Improvements

- File update & delete synchronization
- Frontend chat UI
- Metadata filtering
- Multi-collection support

---

## ğŸ‘¨â€ğŸ’» Author

**Nachiket Shinde**  
AI & ML Engineer | AI Automation  
Founder â€“ KodeNeurons
